{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "catdog_transfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangyao049/Keras-Tutorial/blob/master/catdog_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1NU-Sh7Y8Db",
        "colab_type": "text"
      },
      "source": [
        "# 使用预训练的卷积神经网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtJtM1Ch6CdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# colab连接google drive中的数据\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DIjuQFkY1qY",
        "colab_type": "text"
      },
      "source": [
        "## 1. 在你的数据集上运行卷积基，将输出保存成硬盘中的numpy数组，然后用这个数据作为输入，输入到独立的密集连接分类器中。\n",
        "* 该方法不能使用数据增强\n",
        "* 速度快，使用CPU就可以训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLEgBbuN6Fw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 加载在ImageNet上预训练好的vgg16模型\n",
        "from keras.applications import VGG16\n",
        "\n",
        "base_model = VGG16(weights='imagenet',\n",
        "                   include_top=False,\n",
        "                   input_shape=(150,150,3))\n",
        "\n",
        "base_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUy9uGKhGnRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 利用vgg16网络，对新数据集进行特征提取\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "data_dir = './gdrive/My Drive/cats_and_dogs_small'\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "validation_dir = os.path.join(data_dir, 'validation')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 20\n",
        "\n",
        "# 该函数返回两个值（特征值以及标签）， 特征提取使用base_model.predict\n",
        "def extract_features(dir, sample_count):\n",
        "  features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
        "  labels = np.zeros(shape=(sample_count))\n",
        "  \n",
        "  generator = datagen.flow_from_directory(\n",
        "        dir,\n",
        "        target_size=(150,150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "  \n",
        "  i = 0\n",
        "  for inputs_batch, labels_batch in generator:\n",
        "    features_batch = base_model.predict(inputs_batch)\n",
        "    features[i * batch_size : (i+1) * batch_size] = features_batch\n",
        "    labels[i * batch_size : (i+1) * batch_size] = labels_batch\n",
        "    i += 1\n",
        "    if i * batch_size >= sample_count:\n",
        "      break\n",
        "    \n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbfhTUwlLVHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 对所有图片进行特征提取\n",
        "train_features, train_labels = extract_features(train_dir, 2000)\n",
        "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
        "test_features, test_labels = extract_features(test_dir, 1000)\n",
        "\n",
        "# 将这些特征展平为2D张量，然后使用Dense层进行分类 （Dense层只能处理向量数据，形如（samples，features））\n",
        "train_features = np.reshape(train_features, (2000, 4*4*512))\n",
        "validation_features = np.reshape(validation_features, (1000, 4*4*512))\n",
        "test_features = np.reshape(test_features, (1000, 4*4*512))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adVpFS6LLkbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# 定义一个全连接分类器，并且在刚刚保存的数据和标签上训练这个分类器\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_dim=4*4*512))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=RMSprop(2e-5), loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_features, train_labels, batch_size=20, epochs=30, validation_data=(validation_features, validation_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAVEtg97OAOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_features, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWGRToudQ7rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(test_loss)\n",
        "print(test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq7aNVx1Zbd6",
        "colab_type": "text"
      },
      "source": [
        "## 2. 在顶部添加Dense层来扩展已有模型， 并在输入数据上端到端的运行整个模型。\n",
        "* 该方法可以使用数据增强\n",
        "* 速度慢，计算代价高，需要GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgFdEdbDQ-JZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 在卷积基上添加一个密集分类器\n",
        "from keras.layers import Flatten\n",
        "model2 = Sequential()\n",
        "model2.add(base_model)\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(256, activation='relu'))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Kz6N36R3n4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 在编译和训练模型之前，一定要冻结（freeze）卷积基， 使其在训练过程中保持权重不变。\n",
        "# 如果不这么做，卷积基之前学到的表示将会在训练过程中被修改，因为新添加的Dense层是随机初始化的，所以非常大的权重更新会在网络中传播，对之前学到的表示造成很大的破坏。\n",
        "base_model.trainable = False\n",
        "\n",
        "# 对训练数据进行数据增强\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "# 注意，不能增强验证数据\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "model2.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model2.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUA1txqrbYAV",
        "colab_type": "text"
      },
      "source": [
        "## 3. 微调模型（fine-tuning）\n",
        "* 微调指将其顶部的几层“解冻”，并将这解冻的几层和信增加的部分联合训练。\n",
        "* 只有上面的分类器已经训练好了，才能微调卷积基的顶部几层，如果分类器没有训练好，那么训练期间通过网络传播的误差信号会特别大，微调的几层之前学到的表示都会被破坏。\n",
        "* 微调网络的步骤如下：\n",
        "  1. 在已经训练好的基网络上添加自定义网络。\n",
        "  2. 冻结基网络。\n",
        "  3. 训练所添加的部分。\n",
        "  4. 解冻基网络的一些层。\n",
        "  5. 联合训练解冻的这些层和添加的部分。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq7eEtTBiP6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 冻结直到某一层（block5_onv1之前）的所有层\n",
        "base_model.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in base_model.layers:\n",
        "  if layer.name == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEAeKT4yizjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWn-Y4xHjCTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}