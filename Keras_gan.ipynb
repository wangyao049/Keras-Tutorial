{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_gan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangyao049/Keras-Tutorial/blob/master/Keras_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PROkKCeh_DVU",
        "colab_type": "text"
      },
      "source": [
        "# Training GANs with MNIST dataset, Keras and TensorFlow\n",
        "#### A standard GANs implementations using fully connected layers and the Keras library.\n",
        "\n",
        "* Data\n",
        "\n",
        "  * Rescale the MNIST images to be between -1 and 1.\n",
        "\n",
        "* Generator\n",
        "\n",
        "  * Simple fully connected neural network, LeakyReLU activation and BatchNormalization.\n",
        "  * The input to the generator is called 'latent sample' (100 values) which is a series of randomly generated numbers, and produces 784 (=28x28) data points which represent a digit image. We use the normal distribution.\n",
        "  * The last activation is tanh.\n",
        "  \n",
        "* Discriminator\n",
        "\n",
        "  * Simple fully connected neural network and LeakyReLU activation.\n",
        "  * The last activation is sigmoid.\n",
        "\n",
        "* Loss\n",
        "\n",
        "  * binary_crossentropy\n",
        "\n",
        "* Optimizer\n",
        "\n",
        "  * Adam(lr=0.0002, beta_1=0.5)\n",
        "  * batch_size = 64\n",
        "  * epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCijhYCZCnn-",
        "colab_type": "text"
      },
      "source": [
        "##  1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izaQQpE6APXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91rmcEAuC2Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LeakyReLU, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras import initializers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3F19g24DbVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvJVOc4TDwrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "for i in range(10):\n",
        "  plt.subplot(2, 5, i+1)\n",
        "  x_y = X_train[y_train == i]\n",
        "  plt.imshow(x_y[0], cmap='gray', interpolation='none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN8OZyrLF0wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_train = (X_train.astype('float32') / 255 - 0.5) * 2\n",
        "\n",
        "print('X_train reshape:', X_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQvVoUc4HJlE",
        "colab_type": "text"
      },
      "source": [
        "## 2. Define model\n",
        "\n",
        "Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeyZBMYUHPNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# latent space dimension\n",
        "latent_dim = 100\n",
        "\n",
        "# imagem dimension 28x28\n",
        "img_dim = 784\n",
        "\n",
        "init = initializers.RandomNormal(stddev=0.02)\n",
        "\n",
        "# Generator network\n",
        "generator = Sequential()\n",
        "\n",
        "# Input layer and hidden layer 1\n",
        "generator.add(Dense(128, input_shape=(latent_dim,), kernel_initializer=init))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "# Hidden layer 2\n",
        "generator.add(Dense(256))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "# Hidden layer 3\n",
        "generator.add(Dense(512))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "# Output layer \n",
        "generator.add(Dense(img_dim, activation='tanh'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTeEVeMMLs4b",
        "colab_type": "text"
      },
      "source": [
        " Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjw1jViKq8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Discriminator network\n",
        "discriminator = Sequential()\n",
        "\n",
        "# Input layer and hidden layer 1\n",
        "discriminator.add(Dense(128, input_shape=(img_dim,), kernel_initializer=init))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "# Hidden layer 2\n",
        "discriminator.add(Dense(256))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "# Hidden layer 3\n",
        "discriminator.add(Dense(512))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "# Output layer\n",
        "discriminator.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kt0LkSpOrSJ",
        "colab_type": "text"
      },
      "source": [
        "## 3. Compile model\n",
        "\n",
        "#### Compile discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZY3ER1BOw2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Optimizer\n",
        "optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "discriminator.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNVZp5h8RPHX",
        "colab_type": "text"
      },
      "source": [
        "#### Combined network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA0zLSTGRL1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator.trainable = False\n",
        "\n",
        "d_g = Sequential()\n",
        "d_g.add(generator)\n",
        "d_g.add(discriminator)\n",
        "d_g.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAMaDAsYjMhV",
        "colab_type": "text"
      },
      "source": [
        "## 4. Fit model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dbKcaFORtig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 100\n",
        "batch_size = 64\n",
        "smooth = 0.1\n",
        "\n",
        "real = np.ones(shape=(batch_size, 1))\n",
        "fake = np.zeros(shape=(batch_size, 1))\n",
        "\n",
        "d_loss = []\n",
        "d_g_loss = []\n",
        "\n",
        "for e in range(epochs + 1):\n",
        "    for i in range(len(X_train) // batch_size):\n",
        "        \n",
        "        # Train Discriminator weights\n",
        "        discriminator.trainable = True\n",
        "        \n",
        "        # Real samples\n",
        "        X_batch = X_train[i*batch_size:(i+1)*batch_size]\n",
        "        d_loss_real = discriminator.train_on_batch(x=X_batch, y=real * (1 - smooth))\n",
        "        \n",
        "        # Fake Samples\n",
        "        z = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))\n",
        "        X_fake = generator.predict_on_batch(z)\n",
        "        d_loss_fake = discriminator.train_on_batch(x=X_fake, y=fake)\n",
        "         \n",
        "        # Discriminator loss\n",
        "        d_loss_batch = 0.5 * (d_loss_real[0] + d_loss_fake[0])\n",
        "        \n",
        "        # Train Generator weights\n",
        "        discriminator.trainable = False\n",
        "        d_g_loss_batch = d_g.train_on_batch(x=z, y=real)\n",
        "   \n",
        "        print(\n",
        "            'epoch = %d/%d, batch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, i, len(X_train) // batch_size, d_loss_batch, d_g_loss_batch[0]),\n",
        "            100*' ',\n",
        "            end='\\r'\n",
        "        )\n",
        "    \n",
        "    d_loss.append(d_loss_batch)\n",
        "    d_g_loss.append(d_g_loss_batch[0])\n",
        "    print('epoch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, d_loss[-1], d_g_loss[-1]), 100*' ')\n",
        "\n",
        "    if e % 10 == 0:\n",
        "        samples = 10\n",
        "        x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples, latent_dim)))\n",
        "\n",
        "        for k in range(samples):\n",
        "            plt.subplot(2, 5, k+1)\n",
        "            plt.imshow(x_fake[k].reshape(28, 28), cmap='gray')\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlDtt-3r-tJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting the metrics\n",
        "plt.plot(d_loss)\n",
        "plt.plot(d_g_loss)\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Discriminator', 'Adversarial'], loc='center right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xQ7Z5IG-t7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}